{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import os,PIL\n",
    "import numpy as np\n",
    "digits = []\n",
    "labels = []\n",
    "basewidth = 50\n",
    "count = 0\n",
    "#shift+tab 提示\n",
    "\n",
    "for i in range(65,91):\n",
    "    for img in os.listdir(r'.\\warehouse\\{}'.\\\n",
    "                          format(chr(i))):\n",
    "        pil_img=PIL.Image.open\\\n",
    "        (r'.\\warehouse\\{}\\{}'.format(chr(i),img))\\\n",
    "        .convert('1')\n",
    "        \n",
    "        wprecent = basewidth/float(pil_img.size[0])\n",
    "        hsize = int(float(pil_img.size[1])*wprecent) #这一步非常重要 没有缩放图片训练会很艰难 甚至带不动\n",
    "        img = pil_img.resize((basewidth,hsize),PIL.Image.ANTIALIAS)\n",
    "        \n",
    "        count+=1\n",
    "        digits.append([pixel for pixel in iter(img.getdata())])\n",
    "        labels.append(chr(i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "E:\\python\\lib\\site-packages\\sklearn\\utils\\validation.py:475: DataConversionWarning: Data with input dtype int32 was converted to float64 by StandardScaler.\n",
      "  warnings.warn(msg, DataConversionWarning)\n"
     ]
    }
   ],
   "source": [
    "from  sklearn.preprocessing import StandardScaler\n",
    "import numpy as np\n",
    "digit_ary = np.array(digits)\n",
    "\n",
    "scaler = StandardScaler()\n",
    "scaler.fit(digit_ary)\n",
    "X_scaled = scaler.transform(digit_ary)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MLPClassifier(activation='logistic', alpha=0.0001, batch_size='auto',\n",
       "       beta_1=0.9, beta_2=0.999, early_stopping=False, epsilon=1e-08,\n",
       "       hidden_layer_sizes=(30, 30, 30), learning_rate='constant',\n",
       "       learning_rate_init=0.001, max_iter=10000, momentum=0.9,\n",
       "       nesterovs_momentum=True, power_t=0.5, random_state=None,\n",
       "       shuffle=True, solver='adam', tol=0.0001, validation_fraction=0.1,\n",
       "       verbose=False, warm_start=False)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.neural_network import MLPClassifier\n",
    "mlp = MLPClassifier(hidden_layer_sizes=(30,30,30) ,activation='logistic',max_iter=10000)\n",
    "mlp.fit(X_scaled,labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['captcha.pkl']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.externals import joblib\n",
    "joblib.dump(mlp,'captcha.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "rs = requests.session()\n",
    "\n",
    "with open(r'kaptcha.jpg','wb') as f:\n",
    "    res = rs.get('http://jiaowu.swjtu.edu.cn/servlet/GetRandomNumberToJPEG')\n",
    "    f.write(res.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AxesImage(54,36;334.8x217.44)\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from PIL import Image\n",
    "\n",
    "#读取图片的数字信息\n",
    "pil_image=Image.open(r'kaptcha.jpg').convert('RGB')\n",
    "open_cv_image=np.array(pil_image)\n",
    "print(plt.imshow(open_cv_image)) #绘图"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy \n",
    "from matplotlib import pyplot as plt\n",
    "from datetime import datetime\n",
    "import time \n",
    "import cv2\n",
    "import time\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "basewidth = 50\n",
    "def saveKaptcha(image):\n",
    "    scaler= StandardScaler()\n",
    "    \n",
    "    #读取图片的数字信息\n",
    "    pil_image=Image.open(image).convert('RGB')\n",
    "    open_cv_image=np.array(pil_image)\n",
    "    #print(plt.imshow(open_cv_image)) #绘图\n",
    "\n",
    "\n",
    "    imgray= cv2.cvtColor(open_cv_image,cv2.COLOR_BGR2GRAY)#转灰阶\n",
    "    \n",
    "    \n",
    "    imgray[0:2]=255 #非常重要 图像顶部横线 和左边竖线 去掉 便于识别\n",
    "    imgray[:,0:2]=255 #取一列的方法\n",
    "    ret,thresh= cv2.threshold(imgray,100,255,0)\n",
    "    image,countours,hierarchy = cv2.findContours(thresh,cv2.RETR_TREE,\\\n",
    "                cv2.CHAIN_APPROX_SIMPLE)\n",
    "    cnts= sorted([(c,cv2.boundingRect(c)[0]) for c in countours], key = lambda x: x[1])\n",
    "    ary=[]\n",
    "    for (c,_)  in cnts:\n",
    "        (x,y,w,h)=cv2.boundingRect(c)\n",
    "        #字符高 在15左右 且排除连笔 以此判别 \n",
    "        if 10<h<20 : #不能对宽度进行修饰限定 比如 i 和 w \n",
    "            ary.append((x,y,w,h))\n",
    "            #存图\n",
    "         \n",
    "    for id,(x,y,w,h) in enumerate(ary):\n",
    "        fig=plt.figure()\n",
    "        roi=thresh[y:y+h,x:x+w]\n",
    "        #thre=roi.copy()\n",
    "        plt.imshow(roi)\n",
    "        plt.savefig('.\\workspace\\{}.jpg'.format(id+1),dpi=100)\n",
    "        plt.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "saveKaptcha('kaptcha.jpg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4, 1650)\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "shapes (4,1650) and (1850,30) not aligned: 1650 (dim 1) != 1850 (dim 0)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-72-f694db66bf43>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     17\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata_scaled\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     18\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mclf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata_scaled\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 19\u001b[1;33m \u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-72-f694db66bf43>\u001b[0m in \u001b[0;36mpredict\u001b[1;34m()\u001b[0m\n\u001b[0;32m     16\u001b[0m     \u001b[0mdata_scaled\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mscaler\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     17\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata_scaled\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 18\u001b[1;33m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mclf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata_scaled\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     19\u001b[0m \u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mE:\\python\\lib\\site-packages\\sklearn\\neural_network\\multilayer_perceptron.py\u001b[0m in \u001b[0;36mpredict\u001b[1;34m(self, X)\u001b[0m\n\u001b[0;32m    947\u001b[0m         \"\"\"\n\u001b[0;32m    948\u001b[0m         \u001b[0mcheck_is_fitted\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"coefs_\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 949\u001b[1;33m         \u001b[0my_pred\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_predict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    950\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    951\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mn_outputs_\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mE:\\python\\lib\\site-packages\\sklearn\\neural_network\\multilayer_perceptron.py\u001b[0m in \u001b[0;36m_predict\u001b[1;34m(self, X)\u001b[0m\n\u001b[0;32m    676\u001b[0m                                          layer_units[i + 1])))\n\u001b[0;32m    677\u001b[0m         \u001b[1;31m# forward propagate\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 678\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_forward_pass\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mactivations\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    679\u001b[0m         \u001b[0my_pred\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mactivations\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    680\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mE:\\python\\lib\\site-packages\\sklearn\\neural_network\\multilayer_perceptron.py\u001b[0m in \u001b[0;36m_forward_pass\u001b[1;34m(self, activations)\u001b[0m\n\u001b[0;32m    103\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mn_layers_\u001b[0m \u001b[1;33m-\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    104\u001b[0m             activations[i + 1] = safe_sparse_dot(activations[i],\n\u001b[1;32m--> 105\u001b[1;33m                                                  self.coefs_[i])\n\u001b[0m\u001b[0;32m    106\u001b[0m             \u001b[0mactivations\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mintercepts_\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    107\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mE:\\python\\lib\\site-packages\\sklearn\\utils\\extmath.py\u001b[0m in \u001b[0;36msafe_sparse_dot\u001b[1;34m(a, b, dense_output)\u001b[0m\n\u001b[0;32m    138\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mret\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    139\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 140\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mb\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    141\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    142\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: shapes (4,1650) and (1850,30) not aligned: 1650 (dim 1) != 1850 (dim 0)"
     ]
    }
   ],
   "source": [
    "clf=joblib.load('captcha.pkl')\n",
    "def predict():\n",
    "    data=[]\n",
    "    for img in os.listdir(r'.\\workspace'):\n",
    "        pil_image = PIL.Image.open(r'.\\workspace\\%s'%img).convert('1')\n",
    "        \n",
    "        wpercent=(basewidth/float(pil_image.size[0]))  \n",
    "        hszie=int((float(pil_image.size[1])*float(wpercent))) \n",
    "        img = pil_image.resize((basewidth,hszie),PIL.Image.ANTIALIAS)\n",
    "\n",
    "        data.append([pixel for pixel in iter(img.getdata())])\n",
    "        \n",
    "    \n",
    "    \n",
    "    scaler.fit(data)\n",
    "    data_scaled = scaler.transform(data)\n",
    "    print(data_scaled.shape)\n",
    "    print(clf.predict(data_scaled))\n",
    "predict()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
